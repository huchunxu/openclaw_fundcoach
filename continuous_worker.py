#!/usr/bin/env python3
"""
è¿ç»­æ•°æ®æ‰©å±•å·¥ä½œè„šæœ¬ - 7Ã—24å°æ—¶è¿è¡Œ
è‡ªåŠ¨æ‰©å±•åŸºé‡‘æ•°æ®ç¼“å­˜ï¼Œå®šæœŸæäº¤åˆ°GitHub
"""

import os
import sys
import time
import subprocess
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_evolution.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from enhanced_data_fetcher import EnhancedDataFetcher
from agents.data_backtest.backtest_engine import BacktestEngine


class ContinuousWorker:
    """è¿ç»­å·¥ä½œè€… - 7Ã—24å°æ—¶è¿è¡Œ"""
    
    def __init__(self):
        self.fetcher = EnhancedDataFetcher()
        self.backtest_engine = BacktestEngine()
        self.work_cycle_count = 0
        self.total_funds_processed = 0
        self.last_git_commit_time = None
        
    def get_current_cache_count(self) -> int:
        """è·å–å½“å‰ç¼“å­˜æ–‡ä»¶æ•°é‡"""
        cache_dir = 'data_cache'
        if not os.path.exists(cache_dir):
            return 0
        return len([f for f in os.listdir(cache_dir) if f.endswith('.csv')])
    
    def expand_data_batch(self, batch_size: int = 100) -> int:
        """æ‰©å±•ä¸€æ‰¹æ•°æ®"""
        logger.info(f"å¼€å§‹æ‰©å±•æ•°æ®æ‰¹æ¬¡ï¼Œç›®æ ‡æ•°é‡: {batch_size}")
        
        # è·å–åŸºé‡‘åˆ—è¡¨
        fund_list = self.fetcher.get_comprehensive_fund_list()
        logger.info(f"è·å–åˆ°åŸºé‡‘åˆ—è¡¨: {len(fund_list)} åª")
        
        # è·å–å·²ç¼“å­˜çš„åŸºé‡‘ä»£ç 
        cached_funds = set()
        cache_dir = 'data_cache'
        if os.path.exists(cache_dir):
            for f in os.listdir(cache_dir):
                if f.endswith('.csv'):
                    cached_funds.add(f.replace('.csv', ''))
        
        # é€‰æ‹©æœªç¼“å­˜çš„åŸºé‡‘
        new_funds = []
        for _, row in fund_list.iterrows():
            fund_code = row['fund_code']
            if fund_code not in cached_funds:
                new_funds.append(fund_code)
                if len(new_funds) >= batch_size:
                    break
        
        if not new_funds:
            logger.info("æ‰€æœ‰åŸºé‡‘å·²ç¼“å­˜ï¼Œæ— éœ€æ‰©å±•")
            return 0
        
        logger.info(f"éœ€è¦è·å–æ–°åŸºé‡‘: {len(new_funds)} åª")
        
        # æ‰¹é‡è·å–æ•°æ®
        success_count = 0
        for i, fund_code in enumerate(new_funds):
            try:
                # è·å–æ•°æ®
                nav_data = self.fetcher.fetch_fund_data_with_fallback(fund_code, days=730)
                
                if not nav_data.empty and len(nav_data) > 30:
                    success_count += 1
                    
                # è¿›åº¦æ—¥å¿—
                if (i + 1) % 10 == 0:
                    logger.info(f"è¿›åº¦: {i+1}/{len(new_funds)} ({success_count} æˆåŠŸ)")
                    
            except Exception as e:
                logger.error(f"è·å–åŸºé‡‘{fund_code}æ•°æ®å¤±è´¥: {e}")
                
        logger.info(f"æ‰¹æ¬¡å®Œæˆ: æˆåŠŸè·å– {success_count}/{len(new_funds)} åªåŸºé‡‘")
        return success_count
    
    def run_backtest_on_cache(self) -> dict:
        """å¯¹ç¼“å­˜æ•°æ®è¿è¡Œå›æµ‹"""
        logger.info("å¼€å§‹è¿è¡Œå›æµ‹åˆ†æ...")
        
        cache_dir = 'data_cache'
        if not os.path.exists(cache_dir):
            return {}
            
        # è·å–æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
        cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.csv')]
        if not cache_files:
            return {}
        
        results = {}
        processed = 0
        
        for cache_file in cache_files[:50]:  # æ¯æ¬¡æœ€å¤šå¤„ç†50åª
            fund_code = cache_file.replace('.csv', '')
            try:
                import pandas as pd
                nav_data = pd.read_csv(os.path.join(cache_dir, cache_file), parse_dates=['date'])
                
                if len(nav_data) > 30:
                    backtest_result = self.backtest_engine.backtest_single_fund(fund_code, nav_data)
                    results[fund_code] = backtest_result
                    processed += 1
                    
            except Exception as e:
                logger.error(f"å›æµ‹åŸºé‡‘{fund_code}å¤±è´¥: {e}")
                
        logger.info(f"å›æµ‹å®Œæˆ: {processed} åªåŸºé‡‘")
        return results
    
    def commit_to_github(self, message: str = None) -> bool:
        """æäº¤åˆ°GitHub"""
        try:
            if message is None:
                cache_count = self.get_current_cache_count()
                message = f"auto: Data expansion - {cache_count} funds cached"
            
            # Gitæ“ä½œ
            subprocess.run(['git', 'add', '.'], check=True)
            subprocess.run(['git', 'commit', '-m', message], check=True)
            subprocess.run(['git', 'push', 'origin', 'enhanced-strategy-agent-20260228'], check=True)
            
            self.last_git_commit_time = datetime.now()
            logger.info(f"âœ… æˆåŠŸæäº¤åˆ°GitHub: {message}")
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"æäº¤åˆ°GitHubå¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"Gitæ“ä½œå¼‚å¸¸: {e}")
            return False
    
    def run_work_cycle(self) -> dict:
        """è¿è¡Œä¸€ä¸ªå·¥ä½œå‘¨æœŸ"""
        self.work_cycle_count += 1
        logger.info(f"\n{'='*60}")
        logger.info(f"å·¥ä½œå‘¨æœŸ #{self.work_cycle_count} å¼€å§‹")
        logger.info(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"{'='*60}")
        
        cycle_result = {
            'cycle_id': self.work_cycle_count,
            'start_time': datetime.now().isoformat(),
            'funds_added': 0,
            'backtest_completed': False,
            'git_committed': False
        }
        
        # 1. æ‰©å±•æ•°æ®
        logger.info("ä»»åŠ¡1: æ‰©å±•åŸºé‡‘æ•°æ®...")
        initial_count = self.get_current_cache_count()
        funds_added = self.expand_data_batch(batch_size=50)
        final_count = self.get_current_cache_count()
        
        cycle_result['funds_added'] = final_count - initial_count
        self.total_funds_processed += cycle_result['funds_added']
        
        logger.info(f"æ•°æ®ç¼“å­˜: {initial_count} -> {final_count} (+{cycle_result['funds_added']})")
        
        # 2. è¿è¡Œå›æµ‹
        logger.info("ä»»åŠ¡2: è¿è¡Œå›æµ‹åˆ†æ...")
        backtest_results = self.run_backtest_on_cache()
        cycle_result['backtest_completed'] = len(backtest_results) > 0
        
        # 3. æäº¤åˆ°GitHub
        logger.info("ä»»åŠ¡3: æ£€æŸ¥æ˜¯å¦éœ€è¦æäº¤åˆ°GitHub...")
        should_commit = (
            cycle_result['funds_added'] > 0 or
            self.last_git_commit_time is None or
            (datetime.now() - self.last_git_commit_time).total_seconds() > 3600  # 1å°æ—¶
        )
        
        if should_commit:
            cycle_result['git_committed'] = self.commit_to_github()
        else:
            logger.info("æš‚ä¸éœ€è¦æäº¤åˆ°GitHub")
        
        cycle_result['end_time'] = datetime.now().isoformat()
        
        logger.info(f"\nå·¥ä½œå‘¨æœŸ #{self.work_cycle_count} å®Œæˆ:")
        logger.info(f"  æ–°å¢åŸºé‡‘: {cycle_result['funds_added']}")
        logger.info(f"  æ€»åŸºé‡‘æ•°: {final_count}")
        logger.info(f"  å›æµ‹å®Œæˆ: {cycle_result['backtest_completed']}")
        logger.info(f"  Gitæäº¤: {cycle_result['git_committed']}")
        
        return cycle_result
    
    def run_continuous(self, max_cycles: int = None, rest_seconds: int = 300):
        """è¿ç»­è¿è¡Œ"""
        logger.info("ğŸš€ å¯åŠ¨è¿ç»­å·¥ä½œæ¨¡å¼...")
        logger.info(f"æœ€å¤§å‘¨æœŸæ•°: {max_cycles or 'æ— é™'}")
        logger.info(f"å‘¨æœŸé—´ä¼‘æ¯: {rest_seconds}ç§’")
        
        cycle_count = 0
        while True:
            if max_cycles and cycle_count >= max_cycles:
                logger.info(f"è¾¾åˆ°æœ€å¤§å‘¨æœŸæ•° {max_cycles}ï¼Œåœæ­¢å·¥ä½œ")
                break
                
            try:
                # è¿è¡Œå·¥ä½œå‘¨æœŸ
                self.run_work_cycle()
                cycle_count += 1
                
                # ä¼‘æ¯
                if rest_seconds > 0:
                    logger.info(f"ä¼‘æ¯ {rest_seconds} ç§’...")
                    time.sleep(rest_seconds)
                    
            except KeyboardInterrupt:
                logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢å·¥ä½œ")
                break
            except Exception as e:
                logger.error(f"å·¥ä½œå‘¨æœŸå¼‚å¸¸: {e}")
                logger.info("ä¼‘æ¯60ç§’åç»§ç»­...")
                time.sleep(60)


if __name__ == "__main__":
    worker = ContinuousWorker()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--max-cycles', type=int, default=None, help='æœ€å¤§å·¥ä½œå‘¨æœŸæ•°')
    parser.add_argument('--rest', type=int, default=300, help='å‘¨æœŸé—´ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--single', action='store_true', help='åªè¿è¡Œä¸€ä¸ªå‘¨æœŸ')
    args = parser.parse_args()
    
    if args.single:
        # åªè¿è¡Œä¸€ä¸ªå‘¨æœŸ
        worker.run_work_cycle()
    else:
        # è¿ç»­è¿è¡Œ
        worker.run_continuous(max_cycles=args.max_cycles, rest_seconds=args.rest)