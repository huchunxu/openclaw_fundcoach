#!/usr/bin/env python3
"""
è‡ªåŠ¨è¿›åŒ–å¼•æ“ - å®ç°OpenClaw FundCoachçš„è‡ªè¿›åŒ–èƒ½åŠ›
"""

import pandas as pd
import numpy as np
import os
import sys
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import git
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from enhanced_data_fetcher import EnhancedDataFetcher
from enhanced_backtest_engine import EnhancedBacktestEngine
from agents.strategy_agent.factor_model_enhanced import EnhancedFactorModel
from agents.strategy_agent.fund_scoring_enhanced import EnhancedFundScoringSystem
from agents.portfolio_agent.portfolio_generator_enhanced import EnhancedPortfolioGenerator
from agents.portfolio_agent.weight_optimizer_enhanced import EnhancedWeightOptimizer
from agents.risk_agent.stress_testing_enhanced import EnhancedStressTesting
from agents.risk_agent.risk_exposure_enhanced import EnhancedRiskExposureAnalyzer


class AutoEvolutionEngine:
    """è‡ªåŠ¨è¿›åŒ–å¼•æ“"""
    
    def __init__(self, config_file: str = "evolution_config.json"):
        self.config = self._load_config(config_file)
        self.data_fetcher = EnhancedDataFetcher()
        self.backtest_engine = EnhancedBacktestEngine()
        self.evolution_history = []
        self.repo = git.Repo(os.path.dirname(__file__))
        
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "data_expansion": {
                "target_fund_count": 5000,
                "batch_size": 100,
                "retry_attempts": 3,
                "cache_dir": "data_cache"
            },
            "backtest_settings": {
                "min_history_days": 730,  # 2å¹´
                "max_drawdown_threshold": -0.35,
                "min_sharpe_ratio": 0.8,
                "benchmark_codes": ["000300", "000905"]  # æ²ªæ·±300, ä¸­è¯500
            },
            "evolution_triggers": {
                "sharpe_decline_threshold": 0.1,
                "drawdown_exceed_threshold": True,
                "fund_pool_change_threshold": 0.2,
                "new_factor_available": True
            },
            "github_settings": {
                "auto_commit": True,
                "auto_push": True,
                "branch_prefix": "auto-evolution-",
                "pr_template": "AUTO: Evolution improvement - {improvement_summary}"
            }
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    user_config = json.load(f)
                    # åˆå¹¶é…ç½®
                    for key, value in user_config.items():
                        if key in default_config:
                            default_config[key].update(value)
                        else:
                            default_config[key] = value
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                
        return default_config
    
    def expand_fund_data(self) -> Dict:
        """æ‰©å±•åŸºé‡‘æ•°æ®æ± """
        print("ğŸš€ å¼€å§‹æ‰©å±•åŸºé‡‘æ•°æ®æ± ...")
        
        target_count = self.config["data_expansion"]["target_fund_count"]
        batch_size = self.config["data_expansion"]["batch_size"]
        cache_dir = self.config["data_expansion"]["cache_dir"]
        
        # è·å–ç°æœ‰ç¼“å­˜æ•°æ®
        existing_funds = set()
        if os.path.exists(cache_dir):
            for file in os.listdir(cache_dir):
                if file.endswith('.csv'):
                    fund_code = file.replace('.csv', '')
                    existing_funds.add(fund_code)
        
        print(f"ç°æœ‰åŸºé‡‘æ•°é‡: {len(existing_funds)}")
        
        # è·å–åŸºé‡‘åˆ—è¡¨
        fund_list = self.data_fetcher.get_fund_list_real()
        print(f"è·å–åˆ°åŸºé‡‘åˆ—è¡¨: {len(fund_list)} åª")
        
        # è¿‡æ»¤å·²æœ‰åŸºé‡‘
        new_funds = []
        for _, row in fund_list.iterrows():
            fund_code = row['fund_code']
            if fund_code not in existing_funds:
                new_funds.append(fund_code)
                if len(new_funds) >= (target_count - len(existing_funds)):
                    break
        
        print(f"éœ€è¦è·å–æ–°åŸºé‡‘: {len(new_funds)} åª")
        
        # æ‰¹é‡è·å–æ•°æ®
        success_count = 0
        total_processed = 0
        
        for i in range(0, len(new_funds), batch_size):
            batch = new_funds[i:i+batch_size]
            print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(new_funds)-1)//batch_size + 1} ({len(batch)} åªåŸºé‡‘)")
            
            batch_result = self.data_fetcher.batch_fetch_funds_real(
                batch, 
                use_cache=True,
                max_workers=10,
                timeout=30
            )
            
            success_count += len(batch_result)
            total_processed += len(batch)
            
            # é¿å…è¿‡äºé¢‘ç¹è¯·æ±‚
            time.sleep(2)
        
        # éªŒè¯æ•°æ®è´¨é‡
        final_fund_count = len(existing_funds) + success_count
        data_quality = self._assess_data_quality(cache_dir)
        
        result = {
            "initial_fund_count": len(existing_funds),
            "expanded_fund_count": final_fund_count,
            "success_rate": success_count / max(len(new_funds), 1),
            "data_quality_score": data_quality,
            "cache_dir": cache_dir
        }
        
        print(f"âœ… æ•°æ®æ‰©å±•å®Œæˆ! æœ€ç»ˆåŸºé‡‘æ•°é‡: {final_fund_count}")
        return result
    
    def _assess_data_quality(self, cache_dir: str) -> float:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        if not os.path.exists(cache_dir):
            return 0.0
            
        csv_files = [f for f in os.listdir(cache_dir) if f.endswith('.csv')]
        if not csv_files:
            return 0.0
            
        quality_scores = []
        sample_size = min(10, len(csv_files))
        
        for file in csv_files[:sample_size]:
            try:
                df = pd.read_csv(os.path.join(cache_dir, file))
                if len(df) >= 365:  # è‡³å°‘1å¹´æ•°æ®
                    quality_scores.append(1.0)
                elif len(df) >= 180:  # è‡³å°‘åŠå¹´æ•°æ®
                    quality_scores.append(0.7)
                elif len(df) >= 90:  # è‡³å°‘3ä¸ªæœˆæ•°æ®
                    quality_scores.append(0.4)
                else:
                    quality_scores.append(0.1)
            except:
                quality_scores.append(0.0)
                
        return np.mean(quality_scores) if quality_scores else 0.0
    
    def run_comprehensive_backtest(self, fund_codes: List[str]) -> Dict:
        """è¿è¡Œå…¨é¢å›æµ‹"""
        print("ğŸ“Š å¼€å§‹å…¨é¢å›æµ‹åˆ†æ...")
        
        # è·å–åŸºé‡‘æ•°æ®
        fund_nav_dict = {}
        fund_basic_info = {}
        
        for fund_code in fund_codes:
            nav_data = self.data_fetcher.load_cached_data(fund_code)
            if nav_data is not None and len(nav_data) >= 365:
                fund_nav_dict[fund_code] = nav_data
                
                basic_info = self.data_fetcher.get_fund_basic_info_real(fund_code)
                fund_basic_info[fund_code] = basic_info
        
        if not fund_nav_dict:
            print("âŒ æ— æœ‰æ•ˆåŸºé‡‘æ•°æ®è¿›è¡Œå›æµ‹")
            return {}
        
        # è¿è¡Œå›æµ‹
        backtest_results = {}
        for fund_code, nav_data in fund_nav_dict.items():
            results = self.backtest_engine.backtest_single_fund(fund_code, nav_data)
            backtest_results[fund_code] = results
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        sharpe_ratios = [r['sharpe_ratio'] for r in backtest_results.values()]
        max_drawdowns = [r['max_drawdown'] for r in backtest_results.values()]
        annual_returns = [r['annual_return'] for r in backtest_results.values()]
        
        overall_stats = {
            "total_funds": len(backtest_results),
            "avg_sharpe_ratio": np.mean(sharpe_ratios),
            "median_sharpe_ratio": np.median(sharpe_ratios),
            "best_sharpe_ratio": max(sharpe_ratios),
            "worst_sharpe_ratio": min(sharpe_ratios),
            "avg_max_drawdown": np.mean(max_drawdowns),
            "median_max_drawdown": np.median(max_drawdowns),
            "best_max_drawdown": max(max_drawdowns),  # æœ€å°å›æ’¤ï¼ˆæœ€ä¸è´Ÿé¢ï¼‰
            "worst_max_drawdown": min(max_drawdowns),  # æœ€å¤§å›æ’¤
            "avg_annual_return": np.mean(annual_returns),
            "median_annual_return": np.median(annual_returns)
        }
        
        result = {
            "backtest_results": backtest_results,
            "overall_stats": overall_stats,
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"âœ… å›æµ‹å®Œæˆ! åˆ†æäº† {len(backtest_results)} åªåŸºé‡‘")
        return result
    
    def evaluate_current_performance(self) -> Dict:
        """è¯„ä¼°å½“å‰ç³»ç»Ÿæ€§èƒ½"""
        print("ğŸ” è¯„ä¼°å½“å‰ç³»ç»Ÿæ€§èƒ½...")
        
        # è·å–å½“å‰åŸºé‡‘æ± 
        cache_dir = self.config["data_expansion"]["cache_dir"]
        fund_codes = []
        if os.path.exists(cache_dir):
            for file in os.listdir(cache_dir):
                if file.endswith('.csv'):
                    fund_code = file.replace('.csv', '')
                    fund_codes.append(fund_code)
        
        if not fund_codes:
            return {"status": "no_data", "message": "æ— åŸºé‡‘æ•°æ®"}
        
        # è¿è¡Œå›æµ‹
        backtest_result = self.run_comprehensive_backtest(fund_codes[:1000])  # é™åˆ¶æ•°é‡
        
        if not backtest_result:
            return {"status": "backtest_failed", "message": "å›æµ‹å¤±è´¥"}
        
        # è¯„ä¼°æ€§èƒ½æŒ‡æ ‡
        stats = backtest_result["overall_stats"]
        performance_score = 0.0
        
        # å¤æ™®ç‡è¯„åˆ†
        if stats["avg_sharpe_ratio"] >= 1.0:
            sharpe_score = 1.0
        elif stats["avg_sharpe_ratio"] >= 0.8:
            sharpe_score = 0.8
        elif stats["avg_sharpe_ratio"] >= 0.6:
            sharpe_score = 0.6
        else:
            sharpe_score = 0.4
        
        # å›æ’¤è¯„åˆ†
        avg_dd = stats["avg_max_drawdown"]
        if avg_dd >= -0.2:
            drawdown_score = 1.0
        elif avg_dd >= -0.25:
            drawdown_score = 0.8
        elif avg_dd >= -0.3:
            drawdown_score = 0.6
        else:
            drawdown_score = 0.4
        
        # æ”¶ç›Šè¯„åˆ†
        avg_return = stats["avg_annual_return"]
        if avg_return >= 0.15:
            return_score = 1.0
        elif avg_return >= 0.1:
            return_score = 0.8
        elif avg_return >= 0.05:
            return_score = 0.6
        else:
            return_score = 0.4
        
        performance_score = (sharpe_score + drawdown_score + return_score) / 3.0
        
        evaluation = {
            "status": "success",
            "performance_score": performance_score,
            "metrics": {
                "avg_sharpe_ratio": stats["avg_sharpe_ratio"],
                "avg_max_drawdown": stats["avg_max_drawdown"],
                "avg_annual_return": stats["avg_annual_return"]
            },
            "recommendations": self._generate_recommendations(stats)
        }
        
        print(f"âœ… æ€§èƒ½è¯„ä¼°å®Œæˆ! ç»¼åˆå¾—åˆ†: {performance_score:.2f}")
        return evaluation
    
    def _generate_recommendations(self, stats: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if stats["avg_sharpe_ratio"] < 0.8:
            recommendations.append("å¤æ™®ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–å› å­æ¨¡å‹æˆ–è°ƒæ•´æƒé‡")
            
        if stats["avg_max_drawdown"] < -0.25:
            recommendations.append("å›æ’¤æ§åˆ¶ä¸è¶³ï¼Œå»ºè®®åŠ å¼ºé£é™©çº¦æŸ")
            
        if stats["avg_annual_return"] < 0.08:
            recommendations.append("æ”¶ç›Šè¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®å¼•å…¥æ–°çš„alphaå› å­")
            
        if stats["total_funds"] < 1000:
            recommendations.append("åŸºé‡‘æ± è§„æ¨¡è¾ƒå°ï¼Œå»ºè®®æ‰©å±•æ•°æ®æº")
            
        return recommendations
    
    def trigger_evolution(self, force: bool = False) -> bool:
        """è§¦å‘è¿›åŒ–æµç¨‹"""
        print("ğŸ”„ æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘è¿›åŒ–...")
        
        if force:
            print("âš¡ å¼ºåˆ¶è§¦å‘è¿›åŒ–")
            return True
            
        # è¯„ä¼°å½“å‰æ€§èƒ½
        evaluation = self.evaluate_current_performance()
        if evaluation["status"] != "success":
            print(f"âš ï¸ è¯„ä¼°å¤±è´¥: {evaluation.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return False
            
        performance_score = evaluation["performance_score"]
        metrics = evaluation["metrics"]
        
        # æ£€æŸ¥è¿›åŒ–è§¦å‘æ¡ä»¶
        triggers = self.config["evolution_triggers"]
        
        if triggers["sharpe_decline_threshold"] > 0:
            if metrics["avg_sharpe_ratio"] < (1.0 - triggers["sharpe_decline_threshold"]):
                print("ğŸ¯ è§¦å‘æ¡ä»¶: å¤æ™®ç‡ä¸‹é™")
                return True
                
        if triggers["drawdown_exceed_threshold"]:
            if metrics["avg_max_drawdown"] < -0.3:
                print("ğŸ¯ è§¦å‘æ¡ä»¶: å›æ’¤è¶…å‡ºé˜ˆå€¼")
                return True
                
        if triggers["fund_pool_change_threshold"] > 0:
            # æ£€æŸ¥åŸºé‡‘æ± å˜åŒ–
            cache_dir = self.config["data_expansion"]["cache_dir"]
            current_fund_count = len([f for f in os.listdir(cache_dir) if f.endswith('.csv')])
            if hasattr(self, '_last_fund_count'):
                change_ratio = abs(current_fund_count - self._last_fund_count) / self._last_fund_count
                if change_ratio > triggers["fund_pool_change_threshold"]:
                    print("ğŸ¯ è§¦å‘æ¡ä»¶: åŸºé‡‘æ± æ˜¾è‘—å˜åŒ–")
                    return True
            self._last_fund_count = current_fund_count
            
        if triggers["new_factor_available"]:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å› å­å¯ç”¨
            if hasattr(self, '_last_factor_count'):
                current_factor_count = len(EnhancedFactorModel().__dict__.get('factors', {}))
                if current_factor_count > self._last_factor_count:
                    print("ğŸ¯ è§¦å‘æ¡ä»¶: æ–°å› å­å¯ç”¨")
                    return True
            else:
                self._last_factor_count = 6  # å½“å‰å› å­æ•°é‡
                
        print("â¸ï¸ æ— éœ€è¿›åŒ–ï¼Œå½“å‰æ€§èƒ½è‰¯å¥½")
        return False
    
    def execute_evolution_step(self) -> Dict:
        """æ‰§è¡Œå•æ¬¡è¿›åŒ–æ­¥éª¤"""
        print("ğŸ§¬ æ‰§è¡Œè¿›åŒ–æ­¥éª¤...")
        
        # æ­¥éª¤1: æ‰©å±•æ•°æ®
        data_result = self.expand_fund_data()
        
        # æ­¥éª¤2: é‡æ–°è¯„ä¼°æ€§èƒ½
        evaluation = self.evaluate_current_performance()
        
        # æ­¥éª¤3: å¦‚æœæ€§èƒ½æå‡ï¼Œä¿å­˜ç»“æœ
        evolution_result = {
            "data_expansion": data_result,
            "performance_evaluation": evaluation,
            "timestamp": datetime.now().isoformat(),
            "evolution_id": f"evol_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        }
        
        # ä¿å­˜è¿›åŒ–å†å²
        self.evolution_history.append(evolution_result)
        
        # è‡ªåŠ¨æäº¤åˆ°GitHub
        if self.config["github_settings"]["auto_commit"]:
            self._auto_commit_evolution(evolution_result)
            
        return evolution_result
    
    def _auto_commit_evolution(self, evolution_result: Dict):
        """è‡ªåŠ¨æäº¤è¿›åŒ–ç»“æœåˆ°GitHub"""
        try:
            # åˆ›å»ºæ–°åˆ†æ”¯
            branch_name = f"{self.config['github_settings']['branch_prefix']}{evolution_result['evolution_id']}"
            new_branch = self.repo.create_head(branch_name)
            new_branch.checkout()
            
            # æ·»åŠ å’Œæäº¤æ›´æ”¹
            self.repo.git.add(A=True)
            commit_message = self.config["github_settings"]["pr_template"].format(
                improvement_summary=f"Data expanded to {evolution_result['data_expansion']['expanded_fund_count']} funds, performance score: {evolution_result['performance_evaluation'].get('performance_score', 0):.2f}"
            )
            self.repo.index.commit(commit_message)
            
            # æ¨é€åˆ°è¿œç¨‹
            if self.config["github_settings"]["auto_push"]:
                origin = self.repo.remote(name='origin')
                origin.push(refspec=f"{branch_name}:{branch_name}")
                print(f"âœ… è‡ªåŠ¨æäº¤åˆ°åˆ†æ”¯: {branch_name}")
                
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨æäº¤å¤±è´¥: {e}")
    
    def run_continuous_evolution(self, max_iterations: int = 10):
        """è¿è¡Œè¿ç»­è¿›åŒ–"""
        print("ğŸš€ å¯åŠ¨è¿ç»­è¿›åŒ–å¼•æ“...")
        print("=" * 50)
        
        for iteration in range(max_iterations):
            print(f"\nğŸ”„ è¿›åŒ–è¿­ä»£ {iteration + 1}/{max_iterations}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›åŒ–
            if self.trigger_evolution():
                # æ‰§è¡Œè¿›åŒ–
                result = self.execute_evolution_step()
                print(f"âœ… è¿›åŒ–å®Œæˆ: {result['evolution_id']}")
            else:
                print("â¸ï¸ è·³è¿‡æœ¬æ¬¡è¿­ä»£")
                
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€æ¬¡æ£€æŸ¥
            if iteration < max_iterations - 1:
                wait_time = 3600  # 1å°æ—¶
                print(f"â³ ç­‰å¾… {wait_time} ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡æ£€æŸ¥...")
                time.sleep(wait_time)
        
        print("\nğŸ è¿ç»­è¿›åŒ–å®Œæˆ!")


if __name__ == "__main__":
    # åˆ›å»ºè¿›åŒ–å¼•æ“
    engine = AutoEvolutionEngine()
    
    # è¿è¡Œå•æ¬¡è¿›åŒ–ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if len(sys.argv) > 1 and sys.argv[1] == "--continuous":
        # è¿è¡Œè¿ç»­è¿›åŒ–
        engine.run_continuous_evolution()
    else:
        # è¿è¡Œå•æ¬¡è¿›åŒ–
        result = engine.execute_evolution_step()
        print(f"\nğŸ“Š è¿›åŒ–ç»“æœ:")
        print(f"  æ•°æ®æ‰©å±•: {result['data_expansion']['expanded_fund_count']} åªåŸºé‡‘")
        if 'performance_evaluation' in result:
            eval_result = result['performance_evaluation']
            if eval_result['status'] == 'success':
                print(f"  æ€§èƒ½å¾—åˆ†: {eval_result['performance_score']:.2f}")
                print(f"  å¤æ™®ç‡: {eval_result['metrics']['avg_sharpe_ratio']:.2f}")
                print(f"  å¹³å‡å›æ’¤: {eval_result['metrics']['avg_max_drawdown']:.2%}")